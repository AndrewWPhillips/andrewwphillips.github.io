---
title: "Profile Guided Optimization"
last_modified_at: 2024-01-26T16:20:02+10:00
excerpt: PGO was a great addition to Go 1.21 but you need to know a few things to get the most out of it 
toc: true
toc_sticky: true
categories: [language,pgo]
tags: [optimization,optimisation,pgo,code generation,branch prediction,inline,hot function,cache]
header:
  overlay_image: "/assets/images/torch-light.jpg"
  overlay_filter: 0.2
permalink: /blog/pgo.html
---

**Profile Guided Optimization** (PGO) is a recent addition to Go which allows you to get your code to run faster.  I finally found some time to look at PGO in depth.

You may be thinking: **why bother?** as it is stated that performance is only improved by less than 30%.  Well, if you write code that is used in a lot of "instances" this can still reduce the use of resources (electricity, hardware, etc).  That is good for the budget and the planet.

Disregarding that I have observed that it can provide a noticeable improvement in responsiveness. 

The really nice thing is that (unlike PGO in other languages) **Go makes it easy** to set up and even integrate into an automated build system.  That is, you can profile running production code and use the profile as the input to the next (PGO enabled) build, as long as the code has not changed significantly.

One caveat is that **you must understand the profile collection process**, which I cover below in [Profile Collection Guidance](#profile-collection-guidance) below.

# Background

<details markdown="1">
<summary>General</summary>

**Why Compilers Generate Slow Code**

When I first learnt PDP-11 assembler programming, a lot of my fellow students were saying that it was pointless since soon optimizing (Fortran) compilers would make assembler programming obsolete.  Over the next 4+ decades, lots of people have sais that optimizing compilers (for Fortran, C, Rust, ...) will produce better code, than hand-written assembler.

This may be a forlorn hope.  Benchmarks continue to show that people can always write faster code (at least for small benchmarks).

Of course, for large scale development it is better to use a high-level language.
{: .notice--warning}

There are two reasons.

1. A compiler is a "general purpose" tool since it needs to be able to handle an enormous variety of inputs (source files).  Hand-written assembler can be tailored exactly for the problem at hand.  In other words, something created for a specific purpose is always going to be better than an adapted general purpose solution.  A racing car is faster around a track than a general purpose vehicle.

2. The developer writing the code will have an understanding of how the code will execute which is not conveyed through the source code.  Indeed, an experienced assembler programmer may not even be consciously aware of why she writes code in a particular way.

(Though, thinking about it, maybe these are the same reason stated in 2 different ways.)

As a simple example, consider this Go code that iterates over a slice:

```go
    for _, t := range tags {
        if t == tag {
            break
        }
    }
```

Here is some "pseudo-assembler" generated for this code.

**Note** This code is for a fictitious assembler -- I hope the op-code meanings are obvious: `BEQ` = **branch** if **equal**, etc.  The "variables" `i` and `t` represent CPU registers, while `tag` and `tags` are memory locations.
{: .notice--warning}

```
start:
   MOVE i, 0
   COMP i, len(tags)
   BEQ  end
   MOVE t, tags[i]
   COMP t, tag
   BEQ  end
   INC  i
   BRA  start
end:
   ...
```

This is the obvious mechanical translation, but if it was hand-coded you might know that in production the slice is almost always empty.  Changing the code to use `BNEQ` (branch **not** equal) is better as processors are generally faster for the "untaken" branch. (The instruction pointer (IP register) is just incremented and not loaded with a new address.)

**Note** "Branch optimization" is one of the oldest and most important optimizations used by compilers.  However, modern processors may not benefit so much from this as in the past.
{: .notice--warning}

```
start:
   MOVE i, 0
   COMP i, len(tags)
   BNEQ loop
end:
   ...

loop:
   MOVE t, tags[i]
   COMP t, tag
   BEQ  end
   INC  i
   BRA  start
```

An additional benefit of this code is that the never/rarely executed code (the loop body) can be moved far from the "hot path" through the code.  In other words, unexecuted code (the loop body) does not pollute the instruction cache.  (Modern processors rely heavily on caching as memory speeds have not kept up with increases in CPU speeds.)

**How PGO helps**

There are many such aspects to code generation where a compiler has to determine how to generate code based on some sort of heuristic.  I suspect this is often just guessing but even if based on analysis of lots of real code the compiler's code will always be optimized for the general case.

PGO allows the compiler to generate code optimized to how the actual running code behaves.

**Warning** Of course, PGO, as the name suggest relies on the "profile" used to guide the compiler to be representative of typical running production code.
{: .notice--warning}

In other words PGO allows the compiler, specifically the optimization phase(s), to make more informed decisions about code generation.

---
</details>
<!--****************************-->
<details markdown="1">
<summary>Go</summary>
<br/>

**Problems with Go's Default Inlining**

A function that is inlineable (and does not include the [//go:noinline directive](https://andrewwphillips.github.io/blog/directive.html#gonoinline)) will be inlined at all call sites.

Can you see any problems with this?  I can think of a few:
* a function has a lot of call-sites in the code but has none or few actual calls at run-time will bloat the code
* no account is taken of how often a function is called only its size (if it does not exceed the inline "budget")
* etc

---
</details>
<!--****************************-->
<br/>

# What is PGO?

Compilers optimize the code they generate for improved efficiency.  (This is usually to make the code run **faster** but can also be to use less resources especially **memory** -- though there is often a tradeoff between "space/time" efficiency.)  These optimizations are based on **static analysis** of the code when it is compiled - see **Background/Go** above for more on Go compiler optimizations.

But compilers can generate **even better** results if they know _how_ the code runs in production.  The simplest way to achieve this is to perform **dynamic analysis**, or CPU profiling, of the executing software.  See **Background/General/How PGO Helps** above for details.

Typically, this involves these steps:
1. Compile the software (PGO not enabled)
2. Run the software and generate a CPU profile under typical production conditions
3. Compile the software using this CPU profile
4. Use this version ongoing in production

Also see [Wikipedia](https://en.wikipedia.org/wiki/Profile-guided_optimization).

# Why is Optimization Important?

Performance is not the main focus of the Go language.  On the other hand Go is no slouch either which is why a lot of developers are drawn to it (whence, like me, they discover Go's other great benefits previously unbeknownst to them :).

So, performance is important for many Go developers and PGO is a significant improvement.

For example, I read recently that most of the code running on Google's servers is built using PGO.  When you consider that (my estimate) the electricity bill for all those servers must be approaching (if not exceeding) one billion dollars annually, PGO probably saves Google at least tens of millions of dollars every year.  So it makes sense economically.  It's also **good for the environment**.

# Go's Cool PGO Features

## Uses Existing pprof format

One of the often unrecognised benefits of Go is how many of the different parts -- in the language, runtime, tools, etc -- work so well together.  It is, perhaps, no surprise that Go's PGO integrates well with existing tools and procedures.  For example, **dynamic analysis** of running Go code is simply a matter of generating a **standard CPU profile** (as recently discussed in [CPU Profiling and Flame Graphs](https://andrewwphillips.github.io/blog/flame-graphs.html)).

This means that a lot of tools and knowledge is readily available for use with PGO.  For example, you can use existing tools to manipulate and merge CPU profiles to use with PGO-enabled builds.

## Iterative Profiling

One of the major advantages of Go's PGO is that you can generate profiles from previous PGO-enabled builds, to guide the next build.  That is you can repeat steps 3 and 4 above repeatedly generating new profiles from production to be used with the subsequent build.

This is made possible by two features of Go's implementation of PGO.

First, thanks to some recent Google research, Go's PGO does not suffer from the problem of **iterative instability**.  That is it avoids a type of "feedback" caused when profiling code that has already been PGO-enabled.

Second, it handles code changes better than many PGO implementations, by cleverly (but simply) preventing most code changes from invalidating a CPU profile generated with the previous build.  So you can take a profile of the current production software to guide optimization of the next build, as long as code changes are not large.

**Warning** If you make significant code changes you may need follow all 4 PGO steps above. That is, you may need to profile the latest build of the software without PGO enabled, before rebuilding with PGO enabled.  This may be necessary after a major refactoring of the code but could be needed after something as trivial as global renaming of a **hot function**.
{: .notice--warning}

These innovations are described more (care)fully in [Design and Implementation of PGO for Go](https://go.googlesource.com/proposal/+/master/design/55022-pgo-implementation.md).

## Architecture Independent

Generally you can apply the same CPU profile when compiling for different target environments.  For example, a CPU profile generated on a Linux system should give good results when applied to building the code for Windows, or any other architecture.

**Warning** I suspect this may change with additions to PGO in future Go releases.
{: .notice--warning}

There is one caveat.  Optimizations to code that **targets a specific environment** (e.g. using [Build Tags](https://andrewwphillips.github.io/blog/build-tags.html#targeting-osarch)) are only applied if the CPU profile was obtained from the target environment.  If PGO optimizations are required for different targets then profiles would need to be generated for each environment and applied specifically for each build.

For example, if PGO optimizations were to apply to some file handling code which was OS specific, then CPU profile would have to be generated on each OS.  Thence when each OS specific version is built the relevant CPU profile is used for PGO.

## No Downside

In typical PGO implementations a poor profile can _reduce_ performance.  I have found this is one more disincentive to using PGO.  Why risk it?

Luckily, Go's implementation does not suffer this problem.  So you can use PGO knowing you won't make things slower.  Of course, you should follow the [guidance](#profile-collection-guidance) below to ensure you are getting the benefits.

**Warning** This advantage is mainly a side effect of the types of optimizations that PGO currently performs.  As above, it may not continue as more PGO optimizations are added in the future.

So, let's now look at the current optimizations and some of the likely future improvements.

# PGO Optimizations In Brief

## Inlining of Hot Functions

As I mentioned in the [Background](#background) section above, inlining of functions is one of the most important optimization tricks of the Go compiler.  Of course, not all Go functions can (or should) be inlined, but how the compiler decides what to inline has been refined over the years but does not always produce optimum results.  Basically, the compiler tallies the operations of a function and if it exceeds the "inline budget" then it won't be inlined, otherwise it will.  See Dave Cheney's blog [Mid-stack inlining in Go](https://dave.cheney.net/2020/05/02/mid-stack-inlining-in-go) for a full explanation.

There are several problems with this simplistic approach (see **Problems with Go's Default Inlining** in the **Background** section above).  In brief, it may not inline functions that are called a lot (so-called **hot** functions) if they slightly exceed the budget.  OTOH _unnecessarily_ inlining functions at many call sites results in more memory usage (code bloat) which in turn can reduce the effectiveness of the CPU instruction cache.

PGO does several things to make inlining much more effective.  First, it **increases the inline budget of hot functions**.  It also inlines large hot functions **only at hot call sites** -- ie, it does **not** inline the same function at sites where it is never/rarely called -- greatly reducing code bloat.

Moreover, PGO avoids inlining of functions that the CPU profile reveals to be **cold functions**.  Again, this reduces unnecessary code bloat.

**Note** The `pgoinlinecdfthreshold` command line option tunes whether functions are considered to be hot or not.  The `pgoinlinebudget` option allows you to set the budget for hot functions to be inlined.
{: .notice--info}

## Devirtualizing Method Calls

Calling a method through an interface is slower than a direct method/function call.  A CPU profile can determine that the underlying type used at a particular method call site is always/usually a certain type (or a small number of types).  The compiler can then optimize such call sites to use direct method calls for the specific types, while still allowing for calls through the interface for other types.

This is obviously a good idea, especially for code that uses a lot of interfaces, but it seemed to me there was lower-hanging fruit that could have been tackled first.  It turns out that I was wrong, as **devirtualization** allows further optimizations.  In particular, it can **prevent variables escaping to the heap**, which is a very good thing.

## Code Layout

CPU profiles allow the compiler to find the commonly executed or **hot** paths through the code.  When PGO is enabled the Go compiler uses this information in two ways.  The compiler will **reorder blocks** of code within a function, and (later in the build process) the linker adjust the location of whole functions which determines where they are loaded into instruction memory.

Block reordering is used within a function for better branch optimization.  Even more important it avoids polluting the instruction cache with code that is never/rarely used. See **Why Compilers Generate Slow Code** in the [Background](#background) section above for more details.

Function reordering looks at the whole program.  Without PGO, functions are placed in instruction memory in a fixed order grouped according to their package.  When PGO is enabled functions are moved to improve use of instruction cache.  For example, "hot function" (that could be inlined) is moved as close as possible to its hottest call site.

## Future

The good news is that Go's PGO gets good results with just a few optimizations.  The better news is that there is still a lot of room for more.

First, there are a few refinements that could be performed on the above-mentioned things.  For example, if you are unlucky with your CPU profile some runtime functions could be wrongly reordered by chance and mixed in with unrelated code. This problem is being addressed.

There are also many opportunities for "micro" optimizations like the following.  **Warning** these are based on my understanding of [PGO opportunities umbrella issue](https://github.com/golang/go/issues/62463) and so may be wrong/outdated.

A short loop in a hot code path could be unrolled to avoid loop overhead.  An `if` or `switch` inside a hot loop could be inverted.

If a variable that escapes to the heap, does not need to escape in the **hot path**, then the variable could be allocated on the stack and only moved to the heap if/when the **cold path** is taken.

Generics in Go generate reasonanbly fast code, but sometimes you can't get the best performance due to lack of features like "specialisation" that C++ templates provide.  CPU profiles could determine that a generic function is only used with a specific type for its type parameter in hot path(s).  PGO could recognise this and generate faster "specialised" code for that type only.

See the [PGO opportunities umbrella issue](https://github.com/golang/go/issues/62463) for more.

# A Few Gotchas

## Single Line Statements

Go's CPU profiles only have line number resolution.  Placing too much code on one source file line can hinder PGO.

For code that is called often: avoid calling multiple functions on one line, single-line if statements, single-line functions, etc.

This problem may be addressed in a future Go release - see [cmd/compile: add intra-line discrimination to PGO profiles](https://github.com/golang/go/issues/59612).

## Code Changes Invalidate Profiles

PGO is pretty tolerant of code changes, but sometimes seemingly innocuous changes can cause problems.  

Be careful of global renames, especially of commonly called functions.  This is very easy to do without much thought in many IDEs.

A good practice would be to do all the "superficial" changes (e.g. changes to identifiers) that dos not change the behaviour of the codeat once (and check them into version control together). After that do a "clean" build - ie, build **without PGO enabled**, then produce a clean profile, etc. as in steps 1 to 4 in [What is PGO?](#what-is-pgo) above.

Similarly, do a "clean" build after a major code refactor or if a significant feature has been merged into the main/production branch.

Note that small changes to the source code of a function _won't_ invalidate a CPU profile from the previous build.  Go's PGO can even tolerate a function being moved to a different location or source file as long as it is **not renamed or moved to a different package**.
{: .notice--info}

## Version Control of Profiles

My general rule of thumb is that only files that are modified directly by people should be added to version control.  (A lot of people disagree with this, but I have found it generally avoids confusion.)  However, CPU profiles used by the compiler are an exception to this rule, especially if profiles are generated as part of an automated build process.

If you don't commit the profile used to build the code then if somebody else checks out the code (or you check it out on a different machine) then you will get a different result when you build the code (ie not PGO).

Note that PGO will be silently used if there is a `default.pgo` file in the source code directory.
{: .notice--info}

## Poor Profiles

The way you should generate CPU profiles for PGO may not be the way you are used to generating them.  The accepted advice is they should be **profiles typical of production software**.

For example, you shouldn't use a profile generated from a benchmark.  No matter how important you think a particular function is to the performance, just profiling it will miss the main benefits of PGO.  For example, function reordering (see [Code Layout](#code-layout) above) operates on the whole program.

Another problem occurs with Go software that uses a lot of concurrency -- the behaviour can be inconsistent.  I have found that significantly different CPU profiles are sometimes generated due to unknown factors.

There are also many other aspects to consider.  For example, you may get better results using profiles generated under heavy load than those generated under average load.  Moreover, some types of load, such as HTTP requests, may be more important than others.  See the next section [Profile Collection Guidance](#profile-collection-guidance) for how to generate good profiles.

# Profile Collection Guidance

The efficacy of PGO depends heavily on the CPU profile provided to the compiler.  Here are some tips for getting the best profile for your needs.

In this section I'll occasionally refer to an example of a continuously running server, mainly because it is a **real application on which I have done a lot of PGO testing**, and because it is a fairly typical Go server with different sources of potential load -- HTTP requests coming in from "frontend" web-browsers, and large amounts of "backend" data coming in from various sources.  There are also several "cleanup" tasks that run regularly.

## What is the "Best" Profile?

The general guidance to generate CPU profiles in production under typical conditions.  See [Collecting representative profiles from production](https://go.dev/doc/pgo#:~:text=Collecting%20representative%20profiles%20from%20production)

The problem with this is that it may be difficult to discover and control variables in production.  It may also be difficult to reproduce typical conditions or even determine what they are.  Moreover, "typical" conditions may not even give the best results.

## Eliminating Variables

In many production server environments there may be a lot of things happening which could affect the profile.  You should try to minimise these variables by running on otherwise quiet hardware, but sometimes there are things you are not aware of or cannot control.  It might be better to generate a CPU profile over a longer period of time or merge several profiles (see [Merging Profiles](https://go.dev/doc/pgo#:~:text=.-,Merging%20profiles,-The%20pprof%20tool).

The approach I prefer is to generate profiles in a controlled test environment (see [Controlled Profiling](#controlled-profiling) below).  There is a risk that you have missed something and the test environment does not reflect important aspects of production.  You also must regularly check that changes to the production environment and the software require corresponding changes to the test environment.

## When to Profile

Creating profiles under typical load may not be the best approach since you want the best performance when the software is under heavy load.  In my case the server is typically under light load and I get better worst case request latency if profile(s) are generated under heavy load.

Of course there are problems with generating profiles when the server is under heavy load as the act of generating the profile will slow the server even more. (To be fair, the overhead of generating CPU profiles has improved in recent releases so this is not a major issue, at least for my example.) 

A bigger problem is that the ideal conditions for taking a CPU profile rarely/never occur (or are difficult to detect) in production.  This is another reason to create a controlled test environment as I describe in [Controlled Profiling](#controlled-profiling).

## Cleanup Tasks

My example server regularly runs cleanup tasks (mainly for deleting old things from memory that are no longer being used).  These tasks are not run often and use very little CPU, but they could interfere with the gathering the CPU profile if they happen to run at the wrong time.

Luckily, I have a feature that allows me to manually disable cleanup tasks at any time.  **Warning** If doing turning them off manually don't forget to turn the cleanup tasks back on after creating your CPU profile!

**Warning** On the other hand, if you have cleanup task(s) that run a lot and/or do a lot of work you may want to explicitly **include** them in the CPU profile.  Otherwise, you may risk that they take too long to run and slow everything down.
{: .notice--danger :}

## Garbage Collection

It's been suggested that Go runtime garbage collections interfere with CPU profile collection.  I have not been able to produce any evidence either way in my tests.  However, I don't think it can hurt to turn off the garbage collector (using `debug.SetGCPercent(-1)`) while generating your PGO profile(s).

If you automatically generate CPU profiles as part of CI/CD then you may need to add code that allows cleanup tasks and garbage collections to be controlled by the automated build process.
{: .notice--info :}

## Alternative Targets

As mentioned above in [Arhcitecture Independent](#architecture-independent), you can compile with PGO for multiple targets using the same CPU profile.  If optimization of target specific code is important _must_ generate CPU profiles on each of the important targets.  Thence you can either:

* merge all the profiles into one profile (eg `default.pgo`) to use when building for all targets
* use the CPU profile from each specific target when building that target 

## Controlled Profiling

A better approach than generating CPU profiles in production may be setting up a controlled test environment.  This would allow you to eliminate external influences and set up the type of load where you want the software to perform best.

You may already have a test environment set up (as I do) that can be easily adapted to generate CPU profiles under carefully controlled conditions.
 
In my case, the most important performance aspect of the code is to handle HTTP requests in a timely manner even when under heavy load.  There are also certain backend inputs that have priority.  All other processing, including cleanup tasks can take a back seat. So to set up the "test" server used to generate the CPU profile I would:

* create a large load from a large number of frontend connections (using K6 tool)
* create a smaller load of priority backend inputs
* disable all cleanup tasks

This is run on a separate quiet test server and a CPU profile is generated over 30 seconds.  The profile is then added to version control (as `default.pgo`) and picked up automatically for the next automated production build.

The only drawback is that builds are a bit slower.

With this setup you also need to be aware if the profiling process needs to be adjusted as things change.  You could also set up benchmarks to make sure that PGO is producing useful results...

## Benchmarks

My next task is to set up automated benchmarks at each release.  This would mean creating builds of the same code with and without PGO enabled and run some tests, to warn if PGO does not give latency improvements above a certain threshold (e.g. 20%).  This would provide immediate feedback on problems with the build system especially the generation of CPU profiles.

# Conclusion

PGO is a very useful addition to the arsenal of tools available to gophers.  Further, the Go approach to PGO makes it easy to start using it.  But there are a few things to be aware of, especially in generating CPU profiles, to get the most from PGO.

If you automate the process of generating CPU profiles and check them in to version control they can become a transparent part of the build process.
